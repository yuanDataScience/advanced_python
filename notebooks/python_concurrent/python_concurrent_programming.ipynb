{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77688c28",
   "metadata": {},
   "source": [
    "### This notebook is from a linkedin class on python concurrent programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52a0eff",
   "metadata": {},
   "source": [
    "### sequential/serial execution\n",
    "* program execute a series of instructions sequentially\n",
    "* one instruction is executed at any give moment\n",
    "* speed of the pogram is limited by cpu and how fast it can execute that series of instructions\n",
    "\n",
    "### parallel programming\n",
    "* with multiple processes, the instructions can be broken down into independent parts and executed simultaneously by different processes\n",
    "* components that depend on all parts need the coordinations between the different parts.\n",
    "* extra complexity is added to coordinate the actions, so the processing speed is not linear with the number of processors.\n",
    "* parallel execution increases throughput by\n",
    "  * accomplish a single task faster\n",
    "  * accomplish more tasks in a given time\n",
    "  * scale of the problem that can solve. Big computational tasks have to rely on parallel programming to save time, which outweights the cost of added hardware \n",
    "  \n",
    "### multiprocessor architectures\n",
    "* Flynn's taxonomy (4 classes of computer architecture based on number of concurrent instruction/control streams and number of data streams\n",
    "  * SISD (single instruction single data)\n",
    "    + sequential computer with a single processor unit\n",
    "    + one single instruction at any given moment\n",
    "  * SIMD (single instruction multiple data)  \n",
    "    + parallel computer with multiple processor units\n",
    "    + execute the same instructions at any give momonet, but can operate on different data element\n",
    "    + for example, both executing chopping, one on onion, one on carrot, and their operations are in sync\n",
    "    + suitable for applications that perform the same handful of operations on a massive set of data elements, such as in image analysis. modern computers use GPU with SIMD instructions to do that\n",
    "  * MISD (mutiple instruction, single data)\n",
    "    + each processor unit independently execute its own separate series of instructions, but all of them are operating on the single stream of data.\n",
    "    + not a commonly used architecture\n",
    "  * MIMD (multiple instruction, multiple data)\n",
    "    + multiple processor units. Every processor unit can process a different series of instructions\n",
    "    + at the same time, each of those processors can be operating on a different set of data\n",
    "    + most commonly used architecture in Flynn's taxonomy from multiple core pcs to network clusters in supercomputers.\n",
    "    + separated further into two parallel programming models:\n",
    "      + SPMD (single program, multiple data)\n",
    "        + multiple porcessing units excute a copy of the same single program simultaneously.\n",
    "        + they can each use a different data.\n",
    "        + different from SIMD since in SIMD, processing units execute the same instruction at the same time. In SPMD, procssing units just execute the same program\n",
    "        + the processes can run asynchronously and the program usually includes conditional logic that allows different tasks of the program to only execute the specific parts of the program\n",
    "        + example, two processors execute the same recipe, but can execute the different parts of the recipe\n",
    "        + most common of parallel programming. using multiple processor computer to execute the same program as a MIMD architecture\n",
    "      + MPMD (multiple program, multiple data)\n",
    "        + Each processing unit is processing a different program.\n",
    "        + processors execute indepently on different programs and may on different data. (a head/manager nodes with many worker nodes for function decomposition)\n",
    "* another aspect to conside to categorize computer architectures is based on how memory is organized and how computer access data\n",
    "  + memory opertes at a speed that is usually slower than processor speed.\n",
    "  + when one processor is reading or writing to memory, it only prevents other processors to access that same memory element\n",
    "  + two main memory architecures for parallel computing\n",
    "    + shared memory\n",
    "      + all processors access the same memory with global address space. Although each processor executes its own instructions independently, if one process changes a memory loaction, all the processors will see the change.\n",
    "      + this doesn't mean all the data are on the same physical device. It could be spread across a cluster of systems. The key is all processors see everything happens in the shared memory space.\n",
    "      + shared memory architectures have two categories based on how processors are connected to memory and how fast they can access the memory\n",
    "      + easier to programming since it is easy to access data in shared memory\n",
    "      + difficult to scale since adding more processors to a shared memory system increases the traffic on the shared memory bus and cost to main the cache coherency with communications between all the parts.\n",
    "      + programmer is responsible to synchronize memory accesses to ensure correct behavior.\n",
    "        + uniform memory access (UMA)\n",
    "          + all processors have equal access to the memory and they can access it equally fast.\n",
    "          + Symmetric multiprocessing system (SMP) is a typical UMA architecture.\n",
    "            + two or more identical processor connected to a single shared memory through a system bus (processors connect to cache memory, which connects to system bus, which connects to manin memory, all connections are bi-directional)\n",
    "            + each of processor core of computer or mobile phone is treated as a separate processor as a SMP architecture.\n",
    "              + each core has its own cache as a small, very fast piece of memory that only it can see. The core uses it to store data it frequently works with.\n",
    "              + the challenge is that if a processor copies a copy of data from shared memory and changes it in its local cache, the change needs to be updated back in the shared memory before another processor reads the old value. This issue is called cache coherency. It is handled by the hardware in multicore processors \n",
    "        + nonuniform memory access (NUMA)\n",
    "          + physically connect multiple SMP systems (which is a UMA type architecture) together. The access is non-uniform because some processors will have quicker access to certain parts of the memory than others. (these SMP systems are connected by system bus, and are located on different positions of system bus. It takes longer to access the memory through the bus compared to shared memory within the same SMP). Overall, every processor can still see everything in memory.\n",
    "    + distributed memory\n",
    "      + each processor has its own memory with its own address space and there is no global address space. All processors are connected through some sort of network (such as an ethernet).\n",
    "      + each processor operates independently. if it makes changes to its local memory, that change is not automatically reflected in the memory of other processors. \n",
    "      + it is up to programmer to explicitly define how and when data is communicated between the nodes. (difficult)\n",
    "      + advantage of NUMA is it is scalable\n",
    "        + adding more processors to the system, you get more memory. This makes it cost-effective to use commodity, off-the-shelf computers and network equipment to build large distributed memory systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9955b",
   "metadata": {},
   "source": [
    "### Threads and processes\n",
    "* process:\n",
    "  + when a computer runs an application, that instance of the program executing is referred to as a process\n",
    "    + includes code, data, and state information\n",
    "    + independent instance of a running program\n",
    "    + has its own, separate memory address and space\n",
    "    + can have hundreds of processes at the same time and an operating system's job is to manage all of them\n",
    "    + sharing resouces between processes will need to use inter-process communication(IPC)\n",
    "      + sockets and pipes\n",
    "      + shared memory\n",
    "      + remote procedure calls\n",
    "* within each process, there are one or more smaller sub-elements called threads\n",
    "  + each thread is an independent path of execution through the program\n",
    "  + a different sequence of instructions\n",
    "  + only exists as part of a process (subset of a process)\n",
    "  + basic unit that os manages. Os schedules threads for execution and allocates time on the processor to execute them.\n",
    "  + threads of the same process share the process's address space so they can access to the same resources and memory, including code varialbes, and data, making it easy to work together.\n",
    "  + sharing resources between processes is not as easy as sharing between threads in the same process.\n",
    "  + threads are light-weight and require less overhead to create and terminate\n",
    "  + operation system can switch between threads faster than processes  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c039ded",
   "metadata": {},
   "source": [
    "### concurrency and parallel execution\n",
    "* concurrency: ability of a program to be broken into parts that can be run indepently of each other. These parts can be executed out of order or partially out of order without impacting the result.\n",
    "* independent tasks without multiple processors will be executed by switching back and forth between them, but only one task can be executed at a moment. This may give an illustion of parallel execution, but it is just concurrent execution since only one task is executed at a moment.\n",
    "  + with multiple hardware, such as multiple processors, multiple tasks can be executed simultaneously, then we have parallel execution\n",
    "* concurrency refers to the program structure that enables to deal with multiple things at once\n",
    "* parallelism refers to siumultaneous execution that actually doing multiple things at once\n",
    "* concurrent programming is useful for I/O dependent tasks. when a thread is waiting for I/O response, we can use another thread to accept user's input.\n",
    "* parallel processing is useful for computational intensive tasks, such as matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55e00f",
   "metadata": {},
   "source": [
    "### concurrent python thread\n",
    "* using threads to handle concurrent tasks in python is straightforward.\n",
    "* pyhton interpreter will not allow concurrent threads to execute simultaneously and parallel due to GIL (global interpreter lock)\n",
    "* Global interpreter lock is a mechanism that limits python to only execute one thread at a time when CPython is used as the interpreter\n",
    "* GIL provide a simple way to provide thread-safe memory management for thread-safe operations.\n",
    "* multi-thread is still useful for many I/O bound applications since GIL will not lock threads\n",
    "* for CPU-bound applications, such as intensive computational tasks, GIL can negatively impact performance. \n",
    "  + we can implement parallel algorithms as external library functions such as C++ called by python functions.\n",
    "  + you can also use python multiprocessing package to use multiple processors instead of multiple threads.\n",
    "    + each process will have its a separate interpreter with its own GIL, so different processors can execute simultaneously\n",
    "    + communcations between processors are more difficult than between threads\n",
    "    + uses more resources compared to creating multiple threads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "302a9446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Process ID:  10944\n",
      "Thread Count:  6\n",
      "<_MainThread(MainThread, started 6764)>\n",
      "<Thread(IOPub, started daemon 7740)>\n",
      "<Heartbeat(Heartbeat, started daemon 1356)>\n",
      "<ControlThread(Control, started daemon 8664)>\n",
      "<HistorySavingThread(IPythonHistorySavingThread, started 10936)>\n",
      "<ParentPollerWindows(Thread-4, started daemon 2876)>\n",
      "\n",
      "Starting 12 CPU Wasters...\n",
      "\n",
      "  Process ID:  10944\n",
      "Thread Count:  18\n",
      "<_MainThread(MainThread, started 6764)>\n",
      "<Thread(IOPub, started daemon 7740)>\n",
      "<Heartbeat(Heartbeat, started daemon 1356)>\n",
      "<ControlThread(Control, started daemon 8664)>\n",
      "<HistorySavingThread(IPythonHistorySavingThread, started 10936)>\n",
      "<ParentPollerWindows(Thread-4, started daemon 2876)>\n",
      "<Thread(Thread-5 (cpu_waster), started 3156)>\n",
      "<Thread(Thread-6 (cpu_waster), started 10548)>\n",
      "<Thread(Thread-7 (cpu_waster), started 7372)>\n",
      "<Thread(Thread-8 (cpu_waster), started 7884)>\n",
      "<Thread(Thread-9 (cpu_waster), started 6424)>\n",
      "<Thread(Thread-10 (cpu_waster), started 3352)>\n",
      "<Thread(Thread-11 (cpu_waster), started 9220)>\n",
      "<Thread(Thread-12 (cpu_waster), started 12128)>\n",
      "<Thread(Thread-13 (cpu_waster), started 4492)>\n",
      "<Thread(Thread-14 (cpu_waster), started 5744)>\n",
      "<Thread(Thread-15 (cpu_waster), started 10764)>\n",
      "<Thread(Thread-16 (cpu_waster), started 6212)>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import threading\n",
    "\n",
    "# a simple function that wastes CPU cycles forever\n",
    "def cpu_waster():\n",
    "    while True:\n",
    "        pass\n",
    "\n",
    "# display information about this process\n",
    "print('\\n  Process ID: ', os.getpid())\n",
    "print('Thread Count: ', threading.active_count())\n",
    "for thread in threading.enumerate():\n",
    "    print(thread)\n",
    "\n",
    "print('\\nStarting 12 CPU Wasters...')\n",
    "for i in range(12):\n",
    "    threading.Thread(target=cpu_waster).start()\n",
    "\n",
    "# display information about this process\n",
    "print('\\n  Process ID: ', os.getpid())\n",
    "print('Thread Count: ', threading.active_count())\n",
    "for thread in threading.enumerate():\n",
    "    print(thread)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ebf713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
